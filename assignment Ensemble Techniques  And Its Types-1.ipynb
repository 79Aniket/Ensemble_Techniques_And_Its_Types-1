{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a785864-4ebc-45ba-a310-7a49ee0aaace",
   "metadata": {},
   "source": [
    "## Q1. What is an ensemble technique in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8b3f6b-cc80-4a5a-8cb2-fb1fc3fd6c73",
   "metadata": {},
   "source": [
    "## Ensemble technique in machine learning is a method that combines the predictions of multiple machine learning models to produce a more accurate prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a04c13-675e-4977-8c5e-6cd3c8c477f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db62ca25-004f-4d76-b8ff-3777b59057e7",
   "metadata": {},
   "source": [
    "## Q2. Why are ensemble techniques used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b373e83c-19ae-42e9-8ea4-9d918d77ae5c",
   "metadata": {},
   "source": [
    "## There are two main reasons to use an ensemble over a single model, and they are related; they are: Performance: An ensemble can make better predictions and achieve better performance than any single contributing model. Robustness: An ensemble reduces the spread or dispersion of the predictions and model performance.\n",
    "##  Ensemble techniques are often used to improve the accuracy and robustness of machine learning models, especially on complex tasks with limited data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea2379d-9381-496d-b224-4f5c8851a42a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36055b87-8ec6-421a-a577-5ea114a25b41",
   "metadata": {},
   "source": [
    "## Q3. What is bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c3b7c3-3b89-46b8-8c9c-36cbf1f241c7",
   "metadata": {},
   "source": [
    "## Bagging, also known as Bootstrap aggregation, is an ensemble learning method that looks for different ensemble learners by varying the training dataset. Unlike a single model trained on the entire dataset, bagging creates multiple weak learners or base models trained on a subset of the original dataset. The number of models to use and the size of the subsets is decided by the data scientist building the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b65764-529b-4011-8c85-9862ee7aa95c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0abf87a8-9606-4990-bde4-22ea959eff01",
   "metadata": {},
   "source": [
    "## Q4. What is boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908f00a5-2d06-431e-acaf-ac9f4cae7162",
   "metadata": {},
   "source": [
    "## Boosting is an ensemble technique that looks to change the training data and adjust the weight of the observations based on the previous classification. Unlike the bagging approach, boosting involves dependence on weak learners. The weak learners take the results of the previous weak learner into account and adjust the weights of the data points, which converts the weak learner into a strong learner. Boosting changes the weight associated with an observation that was classified incorrectly by trying to increase the weight associated with it. Boosting tends to decrease the bias error but can sometimes lead to overfitting the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e19d196-f315-4862-b3eb-eaf3bbbc6e10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a31a936-08a7-4917-8da3-c2b0fe8ffda0",
   "metadata": {},
   "source": [
    "## Q5. What are the benefits of using ensemble techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a900578-d354-4990-8a7a-61ece60e365c",
   "metadata": {},
   "source": [
    "## Ensemble techniques offer several advantages :\n",
    "## Improved accuracy\n",
    "## Reduced overfitting\n",
    "## Increased robustness\n",
    "## Interpretability\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f345b22-1af0-42e8-a6d2-f5132a3dae7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3c1748d-01fd-4e42-880f-1fe3542ce6d8",
   "metadata": {},
   "source": [
    "## Q6. Are ensemble techniques always better than individual models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a06ac06-6155-42d2-8b52-bbf8954ec7de",
   "metadata": {},
   "source": [
    "## Ensemble methods have higher predictive accuracy, compared to the individual models. 2. Ensemble methods are very useful when there is both linear and non-linear type of data in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0991abe-dacc-420d-9cfa-26682ace9e37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5813cd57-dee9-4150-a8ac-0620056aa975",
   "metadata": {},
   "source": [
    "## Q7. How is the confidence interval calculated using bootstrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fa269b-72f5-494e-8e3b-4c4f0a52f650",
   "metadata": {},
   "source": [
    "## Sample with replacement from the original data to create a bootstrap sample.\n",
    "## Calculate the statistic of interest on the bootstrap sample. \n",
    "## Repeat steps 1 and 2 many times (e.g., 1000 times) to create a bootstrap distribution of the statistic.\n",
    "## The confidence interval is calculated by finding the quantiles of the bootstrap distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c72351-ef86-4010-be92-883c48e93968",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b5e042a-3a74-4ed8-a884-c5b6eaa1f8c0",
   "metadata": {},
   "source": [
    "## Q8. How does bootstrap work and What are the steps involved in bootstrap?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274249ca-0df5-45ea-9454-7dfbd60bd2e9",
   "metadata": {},
   "source": [
    "## Bootstrapping is a statistical resampling method that uses the data that has already been collected to create new datasets. These new datasets, called bootstrap samples, are used to estimate the distribution of a statistic and its confidence interval.\n",
    "\n",
    "## The steps involved in bootstrapping are as follows:\n",
    "## Sample with replacement from the original data to create a bootstrap sample.\n",
    "## Calculate the statistic of interest on the bootstrap sample. \n",
    "## Repeat steps 1 and 2 many times (e.g., 1000 times) to create a bootstrap distribution of the statistic.\n",
    "## The confidence interval is calculated by finding the quantiles of the bootstrap distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad9ba63-639d-43b0-b16f-5057fc8c6c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
